const generatedBibEntries = {
    "Lewis2020RAG": {
        "abstract": "This paper introduces RAG, a model combining retrieval and generation to enhance knowledge-intensive NLP tasks.",
        "author": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",
        "doi": "10.48550/arXiv.2005.11401",
        "journal": "Advances in neural information processing systems",
        "keywords": "type:Article, Core Methods, RAG, NLP",
        "number": "",
        "publisher": "NeurIPS",
        "series": "NeurIPS",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "url": "https://arxiv.org/abs/2005.11401",
        "volume": "33",
        "year": "2020"
    },
    "Zhang2023ActivePromptRAG": {
        "abstract": "This paper proposes Active-Prompt RAG, a method combining active learning with retrieval-augmented generation for better performance on knowledge-intensive tasks.",
        "author": "Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang",
        "doi": "10.48550/arXiv.2302.12246",
        "journal": "arXiv",
        "keywords": "type:Article, Core Methods, Active Learning, Prompt Engineering, RAG",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "url": "https://arxiv.org/abs/2302.12246",
        "volume": "",
        "year": "2023"
    },
    "Fengbin2021Survey": {
        "abstract": "This survey reviews developments in open-domain QA, highlighting the shift toward integrated retrieval-generation systems.",
        "author": "Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, Tat-Seng Chua",
        "doi": "10.48550/arXiv.2101.00774",
        "journal": "ACM Computing Surveys",
        "keywords": "type:Article, Textual Question Answering, Open Domain Question Answering, Machine Reading Comprehension, Information Retrieval, Natural Language Understanding, Information Extraction",
        "number": "",
        "publisher": "ACM",
        "series": "ACM Computing Surveys",
        "title": "Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering",
        "url": "https://arxiv.org/abs/2101.00774",
        "volume": "",
        "year": "2021"
    },
    "Devlin2019BERT": {
        "abstract": "BERT introduced deep bidirectional transformers, which have become a foundation for modern NLP models including RAG.",
        "author": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
        "doi": "10.18653/v1/N19-1423",
        "journal": "Proceedings of NAACL-HLT",
        "keywords": "type:Inproceedings, Background, NLP, Transformer",
        "number": "",
        "publisher": "NAACL",
        "series": "NAACL",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "url": "https://aclanthology.org/N19-1423/",
        "volume": "",
        "year": "2019"
    },
    "Shi2023REPLUG": {
        "abstract": "REPLUG introduces a retrieval-augmented approach for black-box language models, enabling them to incorporate external knowledge via retrieval without retraining.",
        "author": "Weijia Shi, Xinyun Chen, Yizhong Wang, Yuxiang Wu, Kai-Wei Chang, Caiming Xiong",
        "doi": "10.48550/arXiv.2301.12652",
        "journal": "arXiv",
        "keywords": "type:Article, Core Methods, REPLUG, Retrieval-Augmented, RAG",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "url": "https://arxiv.org/abs/2301.12652",
        "volume": "",
        "year": "2023"
    },
    "Tianxiang2022REPLUG": {
        "abstract": "Introduces a flexible retrieval-augmented framework that enhances black-box language models by integrating external retrieval modules without requiring model fine-tuning.",
        "author": "Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu",
        "doi": "10.48550/arXiv.2201.03514",
        "journal": "arXiv",
        "keywords": "type:Article, Core Methods, REPLUG, Retrieval-Augmented, Black-Box Models",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Black-Box Tuning for Language-Model-as-a-Service",
        "url": "https://arxiv.org/abs/2201.03514",
        "volume": "",
        "year": "2022"
    },
    "Bailicai2024HealthcareRAG": {
        "abstract": "This paper proposes 'Bailicai', a domain-optimized retrieval-augmented generation (RAG) framework designed for healthcare applications. By incorporating four specialized sub-modules, the framework enhances the performance of open-source large language models (LLMs) on medical tasks, reducing hallucinations and improving factual accuracy.",
        "author": "Cui Long, Yongbin Liu, Chunping Ouyang, Ying Yu",
        "doi": "10.48550/arXiv.2407.21055",
        "journal": "arXiv ",
        "keywords": "type:Article, Applications, Healthcare, RAG, Domain Optimization",
        "number": "2407.21055",
        "publisher": "arXiv",
        "series": "arXiv AI",
        "title": "Bailicai: A Domain-Optimized RAG Framework for Healthcare",
        "url": "https://doi.org/10.48550/arXiv.2407.21055",
        "volume": "",
        "year": "2024"
    },

    "Yu2024RAGPlusPlus": {
        "abstract": "RAG++ enhances retrieval-augmented generation by incorporating a reranking module, improving retrieval quality and generation accuracy for knowledge-intensive NLP tasks.",
        "author": "Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou",
        "doi": "10.48550/arXiv.2408.01107",
        "journal": "arXiv",
        "keywords": "type:Article, Core Methods, RAG++, Reranking, Retrieval-Augmented Generation",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "BioRAG: A RAG-LLM Framework for Biological Question Reasoning",
        "url": "https://arxiv.org/abs/2408.01107",
        "volume": "",
        "year": "2024"
    },
    "Izacard2022Atlas": {
        "abstract": "Atlas presents a few-shot learning approach using retrieval-augmented language models, significantly improving performance on knowledge-intensive NLP tasks.",
        "author": "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, Edouard Grave",
        "doi": "10.48550/arXiv.2208.03299",
        "journal": "arXiv",
        "keywords": "type:Article, Core Methods, Atlas, Few-Shot, Retrieval-Augmented Models",
        "number": "",
        "publisher": "arXiv",
        "series": "arXiv",
        "title": "Atlas: Few-shot Learning with Retrieval-Augmented Language Models",
        "url": "https://arxiv.org/abs/2208.03299",
        "volume": "",
        "year": "2022"
    },
    "Tran2024HealthcareRAG": {
        "abstract": "This paper explores the application of retrieval-augmented generation (RAG) techniques in healthcare, focusing on improving medical question answering and decision support systems.",
        "author": "Muhammad Arslan, Hussam Ghanem, Saba Munawar, Christophe Cruz",
        "doi": "10.1016/j.procs.2024.09.178",
        "journal": "Procedia Computer Science",
        "keywords": "type:Article, Applications, Healthcare, Retrieval-Augmented Generation, Medical QA",
        "number": "",
        "publisher": "Elsevier",
        "series": "Procedia Computer Science",
        "title": "A Survey on RAG with LLMs",
        "url": "https://doi.org/10.1016/j.procs.2024.09.178",
        "volume": "",
        "year": "2024"
    }
};

